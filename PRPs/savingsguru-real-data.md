name: "SavingsGuru Real Data Implementation - Context-Rich PRP"
description: |

## Goal
Transform a React deals website that currently scrapes SavingsGuru.ca and generates **completely FAKE pricing data** into a production system that provides **100% real Amazon pricing data** using Amazon Product Advertising API (PAAPI) with web scraping fallback. Build a complete full-stack application with polished React GUI.

## Why
- **Data Integrity**: Current system generates fake prices, misleading users and damaging trust
- **Compliance**: Using real Amazon data ensures accuracy and maintains affiliate relationship integrity
- **User Experience**: Real product images, titles, and discounts provide authentic shopping experience
- **Revenue Protection**: Accurate pricing prevents lost affiliate commissions from incorrect data

## What
Build a complete full-stack deals aggregation system that:

### Backend Requirements (Python)
- **Amazon API Integration**: Use `python-amazon-paapi` library for real product data
- **Web Scraping Fallback**: Scrape live Amazon.ca pages when API fails with proper bot detection avoidance
- **Data Processing**: Update `focused_scraper.py` to eliminate fake data generation
- **Robust Error Handling**: Graceful failures with comprehensive logging

### Frontend Requirements (React + TypeScript + Tailwind)
- **Static React App**: Create React App with TypeScript, reads from `public/deals.json`
- **Responsive Design**: Mobile-first with desktop enhancements
- **Interactive Components**: Deal cards, modals, sidebar with real pricing display
- **Performance**: Lazy loading, smooth animations, masonry/grid layout

### Success Criteria
- [ ] **Zero fake prices** - every price from Amazon API or live scraping
- [ ] **Real product images** - direct from Amazon servers
- [ ] **Real product titles** - from Amazon, not SavingsGuru post titles
- [ ] **Real discounts** - calculated from actual list vs current price
- [ ] **Robust fallbacks** - API failure → live scraping → skip product (no fake data)
- [ ] **Production-ready frontend** - responsive, performant, polished UI

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- url: https://python-amazon-paapi.readthedocs.io/
  why: Core Python library for PAAPI integration, setup patterns, error handling
  
- url: https://webservices.amazon.com/paapi5/documentation/
  why: Official Amazon PAAPI 5.0 specs, authentication, rate limits, response formats
  
- url: https://github.com/sergioteula/python-amazon-paapi
  why: Library GitHub with latest examples, Canadian marketplace setup
  
- file: use-cases/pydantic-ai/examples/main_agent_reference/settings.py
  why: Environment variable loading pattern with pydantic-settings and python-dotenv

- file: use-cases/pydantic-ai/examples/testing_examples/test_agent_patterns.py
  why: Testing patterns with pytest, async/await, mock usage
  
- file: use-cases/mcp-server/src/database/utils.ts
  why: Error handling patterns, logging, timing measurement for operations

- doc: https://dev.to/kdrbek/responsive-masonry-layout-with-tailwindcss-in-2-steps-4kkj
  section: Responsive masonry layouts with Tailwind CSS columns
  critical: Modern CSS columns approach for card layouts without JavaScript libraries
  
- doc: https://www.zenrows.com/blog/stealth-web-scraping-in-python-avoid-blocking-like-a-ninja
  section: Anti-bot detection techniques for Amazon scraping
  critical: User-Agent rotation, request delays, session management to avoid blocking

- doc: https://flowbite.com/docs/components/card/
  section: Card components with Tailwind CSS
  critical: Modern card design patterns for deals/product display
```

### Current Codebase Tree
```bash
# Based on INITIAL.md requirements
scraper/
├── focused_scraper.py          # NEEDS MODIFICATION - remove fake data generation
├── amazon_api.py               # TO CREATE - PAAPI integration
└── requirements.txt            # TO UPDATE - add python-amazon-paapi

src/
├── App.tsx                     # TO CREATE - Main app orchestrator
├── components/
│   ├── DealCard.tsx           # TO CREATE - Individual deal display
│   ├── DealModal.tsx          # TO CREATE - Deal detail popup
│   ├── Header.tsx             # TO CREATE - Site navigation
│   ├── Sidebar.tsx            # TO CREATE - Featured deals
│   └── HomePage.tsx           # TO CREATE - Main grid layout
├── types/
│   └── Deal.ts                # TO CREATE - TypeScript interfaces
├── utils/
│   ├── dealUtils.ts           # TO CREATE - Price formatting utilities
│   └── priceVisibility.ts     # TO CREATE - Price display logic
└── index.css                  # TO CREATE - Tailwind imports

public/
└── deals.json                 # GENERATED BY - scraper output

package.json                   # TO CREATE - React TypeScript project
```

### Desired Codebase Tree with Files to be Added and Responsibility of File
```bash
scraper/
├── amazon_api.py              # PAAPI integration, Canadian marketplace, error handling
├── scraper_fallback.py        # Live Amazon.ca scraping with anti-bot measures
├── focused_scraper.py         # MODIFIED - use real data sources only
├── settings.py                # Environment configuration with pydantic-settings
├── models.py                  # Pydantic models for data validation
├── utils.py                   # Shared utilities, logging, data formatting
├── requirements.txt           # Updated dependencies
└── tests/
    ├── test_amazon_api.py     # PAAPI integration tests
    ├── test_scraper.py        # Scraping tests with mocked responses
    └── conftest.py            # Pytest configuration

# React Frontend (Complete new structure)
package.json                   # CRA with TypeScript, Tailwind dependencies
public/
├── deals.json                 # Generated by scraper - static data file
└── index.html                 # CRA generated

src/
├── App.tsx                    # Main app with deal loading, modal state management
├── index.tsx                  # React entry point
├── index.css                  # Tailwind imports and custom styles
├── components/
│   ├── DealCard.tsx          # Individual deal card with price display, color rotation
│   ├── DealModal.tsx         # Full-screen/sidebar modal for deal details
│   ├── Header.tsx            # Responsive header with branding
│   ├── Sidebar.tsx           # Featured deals list (desktop only)
│   └── HomePage.tsx          # Main grid layout with masonry styling
├── types/
│   └── Deal.ts               # TypeScript interfaces for deal data
├── utils/
│   ├── dealUtils.ts          # Price formatting, discount calculation utilities
│   ├── priceVisibility.ts    # Logic for showing/hiding prices on cards
│   └── constants.ts          # Color schemes, breakpoints, configuration
├── hooks/
│   ├── useDeals.ts           # Custom hook for loading deals data
│   └── useModal.ts           # Modal state management
└── __tests__/
    ├── components/           # Component unit tests
    └── utils/                # Utility function tests

# Configuration files
tailwind.config.js            # Tailwind CSS configuration
tsconfig.json                 # TypeScript configuration
.gitignore                    # Ignore node_modules, build files
.env.example                  # Environment variables template
README.md                     # Setup and deployment instructions
```

### Known Gotchas of our Codebase & Library Quirks
```python
# CRITICAL: python-amazon-paapi requires specific setup for Canadian marketplace
from amazon_paapi import AmazonApi

# GOTCHA: Must specify 'CA' for Canadian marketplace, not 'Canada'
amazon = AmazonApi(key=ACCESS_KEY, secret=SECRET_KEY, tag=PARTNER_TAG, country='CA')

# CRITICAL: PAAPI has rate limits - 1 request per second for free tier
# Need to implement proper throttling between API calls
import time
time.sleep(1)  # Minimum 1 second between requests

# GOTCHA: PAAPI returns different response structure than v4
# Use 'ASIN', 'ItemInfo.Title', 'Offers.Listings[0].Price' paths
response = amazon.get_items(item_ids)
title = response['ItemsResult']['Items'][0]['ItemInfo']['Title']['DisplayValue']

# CRITICAL: Web scraping Amazon requires specific headers to avoid 403
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

# GOTCHA: We use pydantic v2 and python-dotenv pattern
from pydantic_settings import BaseSettings
from dotenv import load_dotenv
load_dotenv()  # Must be called before Settings initialization

# CRITICAL: React masonry layout using CSS columns, not JavaScript
# Tailwind config needs custom utilities for masonry
.masonry {
  column-count: 2;
  column-gap: 1rem;
}
.masonry-sm { column-count: 1; }
.masonry-md { column-count: 3; }

# GOTCHA: TypeScript in CRA requires @types packages for all dependencies
npm install --save-dev @types/react @types/react-dom @types/node
```

## Implementation Blueprint

### Data Models and Structure

Create the core data models to ensure type safety and consistency:

```python
# Python Pydantic models for data validation
from pydantic import BaseModel, HttpUrl, Field
from typing import Optional, List
from decimal import Decimal
from datetime import datetime

class AmazonProduct(BaseModel):
    """Amazon product data from PAAPI or scraping"""
    asin: str = Field(..., description="Amazon ASIN")
    title: str = Field(..., description="Product title from Amazon")
    current_price: Optional[Decimal] = Field(None, description="Current price in CAD")
    list_price: Optional[Decimal] = Field(None, description="Original/list price in CAD")
    discount_percent: Optional[int] = Field(None, description="Calculated discount percentage")
    image_url: Optional[HttpUrl] = Field(None, description="High-res product image URL")
    availability: str = Field(default="Unknown", description="Product availability status")
    prime_eligible: bool = Field(default=False, description="Prime shipping eligibility")
    
class Deal(BaseModel):
    """Final deal object for frontend consumption"""
    id: str
    title: str
    image_url: str
    price: float  # Current price
    original_price: Optional[float]  # List price
    discount_percent: Optional[int]
    category: str
    description: str
    affiliate_url: str  # Amazon.ca URL with affiliate tag
    featured: bool
    date_added: str  # ISO format
    data_source: str  # "PAAPI", "SCRAPED", or "FALLBACK"
```

```typescript
// TypeScript interfaces for React frontend
interface Deal {
  id: string;
  title: string;
  imageUrl: string;
  price: number;
  originalPrice?: number;
  discountPercent?: number;
  category: string;
  description: string;
  affiliateUrl: string;
  featured: boolean;
  dateAdded: string;
  dataSource: 'PAAPI' | 'SCRAPED' | 'FALLBACK';
}

interface DealsState {
  deals: Deal[];
  loading: boolean;
  error: string | null;
  selectedDeal: Deal | null;
}
```

### List of Tasks to be Completed to Fulfill the PRP in Order

```yaml
Task 1: Setup Python Environment & Dependencies
MODIFY scraper/requirements.txt:
  - ADD python-amazon-paapi>=5.0.0
  - ADD pydantic-settings>=2.0.0
  - ADD python-dotenv>=1.0.0
  - ADD httpx>=0.27.0  # for async scraping
  - ADD beautifulsoup4>=4.12.0
  - ADD pytest>=7.0.0
  - PRESERVE existing requirements if any

CREATE scraper/settings.py:
  - MIRROR pattern from: use-cases/pydantic-ai/examples/main_agent_reference/settings.py
  - MODIFY for Amazon API credentials (AMZ_ACCESS_KEY, AMZ_SECRET_KEY, AMZ_PARTNER_TAG)
  - KEEP pydantic-settings and python-dotenv pattern

Task 2: Create Amazon PAAPI Integration Module
CREATE scraper/amazon_api.py:
  - IMPLEMENT AmazonAPIClient class with Canadian marketplace setup
  - ADD rate limiting (1 request per second minimum)
  - ADD error handling for API failures, network issues
  - ADD product data extraction methods (get_product_info, search_products)
  - USE async/await patterns for performance
  - RETURN structured AmazonProduct models

Task 3: Create Web Scraping Fallback Module  
CREATE scraper/scraper_fallback.py:
  - IMPLEMENT anti-bot measures (User-Agent rotation, request delays)
  - ADD Amazon.ca product page scraping
  - ADD price extraction from HTML (current price, crossed-out price)
  - ADD image and title extraction
  - USE session management for cookies
  - IMPLEMENT exponential backoff for failed requests

Task 4: Create Data Models and Validation
CREATE scraper/models.py:
  - IMPLEMENT AmazonProduct, Deal, and other Pydantic models
  - ADD validation for prices, URLs, required fields
  - ADD data transformation methods (Amazon API → Deal format)
  - ADD error handling for malformed data

Task 5: Update Main Scraper Logic
MODIFY scraper/focused_scraper.py:
  - FIND and REMOVE generate_canadian_pricing() function completely
  - REPLACE fake data generation with real Amazon API calls
  - ADD fallback chain: PAAPI → web scraping → skip product
  - ADD comprehensive logging for data sources
  - ADD data integrity checks (skip products without real pricing)
  - PRESERVE existing SavingsGuru.ca scraping logic for ASIN extraction

Task 6: Create React TypeScript Project
CREATE package.json:
  - USE Create React App with TypeScript template
  - ADD Tailwind CSS dependencies
  - ADD testing libraries (jest, @testing-library/react)
  - ADD additional dependencies for lazy loading, animations

RUN setup commands:
  - npx create-react-app . --template typescript
  - npm install -D tailwindcss postcss autoprefixer
  - npx tailwindcss init -p

Task 7: Implement React Components
CREATE src/types/Deal.ts:
  - IMPLEMENT Deal interface matching Python Deal model
  - ADD utility types for component props

CREATE src/components/DealCard.tsx:
  - IMPLEMENT responsive card with color rotation (pink, blue, yellow)
  - ADD price display with strikethrough original price
  - ADD discount badge for 40%+ discounts
  - ADD click handler for modal opening
  - ADD lazy loading for images with error fallbacks

CREATE src/components/DealModal.tsx:
  - IMPLEMENT full-screen modal on mobile, sidebar on desktop
  - ADD large product image, full title, description
  - ADD prominent pricing with CTA button
  - ADD category chips and close functionality

CREATE src/components/Header.tsx:
  - IMPLEMENT responsive header with SavingsGuru branding
  - ADD mobile-responsive navigation if needed

CREATE src/components/Sidebar.tsx:
  - IMPLEMENT featured deals list (top 20 by discount)
  - ADD compact layout with small images and prices
  - HIDE on mobile devices

CREATE src/components/HomePage.tsx:
  - IMPLEMENT masonry/grid layout for deal cards
  - ADD responsive breakpoints and card distribution
  - ADD loading states and error handling

Task 8: Implement App Logic and State Management
CREATE src/App.tsx:
  - IMPLEMENT main app orchestrator
  - ADD deal loading from public/deals.json
  - ADD modal state management
  - ADD responsive layout with sidebar and main grid
  - ADD error boundaries and loading states

CREATE src/utils/dealUtils.ts:
  - IMPLEMENT price formatting utilities (Canadian dollars)
  - ADD discount calculation functions
  - ADD date formatting and sorting utilities

CREATE src/utils/priceVisibility.ts:
  - IMPLEMENT logic for showing prices on ~10% of cards
  - ADD "Check Price" button logic for cards without visible prices

Task 9: Styling and Responsive Design
MODIFY src/index.css:
  - ADD Tailwind CSS imports (@tailwind base, components, utilities)
  - ADD custom masonry utilities for CSS columns
  - ADD glassmorphism effects for modals
  - ADD smooth animations and transitions

CREATE tailwind.config.js:
  - ADD custom colors for card rotation (pink, blue, yellow)
  - ADD responsive breakpoints
  - ADD custom utilities for masonry layout

Task 10: Testing Implementation
CREATE scraper/tests/test_amazon_api.py:
  - IMPLEMENT tests for PAAPI integration with mock responses
  - ADD tests for rate limiting and error handling
  - ADD tests for Canadian marketplace specific behavior

CREATE scraper/tests/test_scraper.py:
  - IMPLEMENT tests for web scraping fallback
  - ADD tests for anti-bot measures
  - ADD integration tests with real Amazon pages (limited)

CREATE src/__tests__/components/:
  - IMPLEMENT unit tests for React components
  - ADD tests for responsive behavior
  - ADD tests for modal interactions and price display logic

Task 11: Integration and Data Pipeline
CREATE integration between scraper and React app:
  - ENSURE scraper outputs deals.json to public/ folder
  - ADD data validation between Python output and TypeScript consumption
  - ADD error handling for malformed or missing data

Task 12: Deployment Configuration
CREATE deployment configuration:
  - ADD Vercel configuration for static deployment
  - ADD environment variable setup instructions
  - ADD CI/CD pipeline for automatic rebuilds
  - ADD monitoring and error tracking setup
```

### Per Task Pseudocode for Critical Tasks

```python
# Task 2: Amazon PAAPI Integration
class AmazonAPIClient:
    def __init__(self, settings: Settings):
        # PATTERN: Use pydantic settings for configuration
        self.amazon = AmazonApi(
            key=settings.amz_access_key,
            secret=settings.amz_secret_key,
            tag=settings.amz_partner_tag,
            country='CA'  # CRITICAL: Canadian marketplace
        )
        self._last_request_time = 0
    
    async def get_product_info(self, asin: str) -> Optional[AmazonProduct]:
        # CRITICAL: Rate limiting for PAAPI
        await self._throttle_request()
        
        try:
            # GOTCHA: PAAPI v5 response structure
            response = self.amazon.get_items(asin, resources=['ItemInfo.Title', 'Offers.Listings', 'Images.Primary'])
            item = response['ItemsResult']['Items'][0]
            
            # PATTERN: Extract data with safe navigation
            title = item.get('ItemInfo', {}).get('Title', {}).get('DisplayValue', '')
            current_price = self._extract_price(item.get('Offers', {}))
            image_url = self._extract_image(item.get('Images', {}))
            
            return AmazonProduct(
                asin=asin,
                title=title,
                current_price=current_price,
                image_url=image_url
            )
        except Exception as e:
            # PATTERN: Log and return None for graceful fallback
            logger.error(f"PAAPI failed for {asin}: {e}")
            return None

# Task 3: Web Scraping Fallback
class AmazonScrapingClient:
    def __init__(self):
        self.session = httpx.AsyncClient(
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                # CRITICAL: Full browser headers to avoid bot detection
            },
            timeout=httpx.Timeout(30.0)
        )
    
    async def scrape_product(self, asin: str) -> Optional[AmazonProduct]:
        url = f"https://www.amazon.ca/dp/{asin}"
        
        try:
            # PATTERN: Retry with exponential backoff
            for attempt in range(3):
                await asyncio.sleep(random.uniform(1, 3))  # Random delay
                response = await self.session.get(url)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    return self._extract_product_data(soup, asin)
                elif response.status_code == 503:
                    # GOTCHA: Amazon returns 503 for rate limiting
                    await asyncio.sleep(2 ** attempt)
                    continue
                else:
                    break
            
            return None
        except Exception as e:
            logger.error(f"Scraping failed for {asin}: {e}")
            return None
```

```typescript
// Task 7: React Components
const DealCard: React.FC<{ deal: Deal; onClick: () => void }> = ({ deal, onClick }) => {
  // PATTERN: Color rotation for card backgrounds
  const colorIndex = parseInt(deal.id.slice(-1), 10) % 3;
  const bgColors = ['bg-pink-100', 'bg-blue-100', 'bg-yellow-100'];
  
  // PATTERN: Price visibility logic
  const showPrice = usePriceVisibility(deal.id);
  
  return (
    <div 
      className={`${bgColors[colorIndex]} rounded-lg shadow-md break-inside-avoid mb-4 cursor-pointer`}
      onClick={onClick}
    >
      {/* CRITICAL: Lazy loading for performance */}
      <img 
        src={deal.imageUrl} 
        alt={deal.title}
        loading="lazy"
        className="w-full h-auto rounded-t-lg"
        onError={(e) => {
          // PATTERN: Fallback to placeholder on image error
          e.currentTarget.src = '/placeholder-image.jpg';
        }}
      />
      
      <div className="p-4">
        <h3 className="font-semibold text-gray-800">{deal.title}</h3>
        
        {showPrice ? (
          <div className="mt-2">
            <span className="text-lg font-bold text-green-600">${deal.price}</span>
            {deal.originalPrice && (
              <span className="ml-2 text-sm line-through text-gray-500">
                ${deal.originalPrice}
              </span>
            )}
          </div>
        ) : (
          <button className="mt-2 bg-blue-500 text-white px-4 py-2 rounded">
            Check Price
          </button>
        )}
        
        {/* CRITICAL: Only show discount badge for significant discounts */}
        {deal.discountPercent && deal.discountPercent >= 40 && (
          <span className="inline-block bg-red-500 text-white px-2 py-1 rounded-full text-xs mt-2">
            {deal.discountPercent}% OFF
          </span>
        )}
      </div>
    </div>
  );
};
```

### Integration Points
```yaml
DATA_PIPELINE:
  - source: "scraper/focused_scraper.py outputs deals.json"
  - destination: "public/deals.json for React consumption"
  - validation: "Pydantic models → TypeScript interfaces"
  
ENVIRONMENT:
  - python: ".env file with AMZ_ACCESS_KEY, AMZ_SECRET_KEY, AMZ_PARTNER_TAG"
  - react: "No environment variables needed (static deployment)"
  
API_INTEGRATION:
  - primary: "Amazon PAAPI via python-amazon-paapi library"
  - fallback: "Direct Amazon.ca scraping with anti-bot measures"
  - affiliate: "Amazon.ca URLs with savingsgurucc-20 tag"
```

## Validation Loop

### Level 1: Python Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
cd scraper/
python -m pip install -r requirements.txt

# Type checking and linting
ruff check . --fix      # Auto-fix what's possible
mypy .                  # Type checking for Python files

# Expected: No errors. If errors, READ the error and fix.
```

### Level 2: Python Unit Tests
```python
# CREATE test_amazon_api.py, test_scraper.py with these patterns:
def test_paapi_integration():
    """Test PAAPI with real Canadian marketplace"""
    client = AmazonAPIClient(settings)
    # Use a known Canadian ASIN for testing
    product = await client.get_product_info("B08N5WRWNW")  # Echo Dot
    assert product is not None
    assert product.title is not None
    assert "amazon.ca" in str(product.image_url) if product.image_url else True

def test_scraping_fallback():
    """Test fallback scraping with anti-bot measures"""
    scraper = AmazonScrapingClient()
    product = await scraper.scrape_product("B08N5WRWNW")
    # Should either succeed or fail gracefully
    assert product is None or isinstance(product, AmazonProduct)

def test_no_fake_data_generation():
    """Critical test: ensure no fake data is generated"""
    scraper = FocusedScraper()
    deals = scraper.scrape_deals()
    
    for deal in deals:
        # CRITICAL: Every deal must have a real data source
        assert deal.data_source in ["PAAPI", "SCRAPED"]
        # No placeholder or generated prices
        if deal.price:
            assert deal.price > 0
            assert "fake" not in deal.data_source.lower()
```

```bash
# Run and iterate until passing:
pytest scraper/tests/ -v --tb=short

# If failing: Read error, understand root cause, fix code, re-run
# NEVER mock real API calls in integration tests - use rate limiting
```

### Level 3: React Development & Testing
```bash
# Frontend setup and validation
npm install
npm run build       # Ensure TypeScript compiles without errors

# Type checking
npx tsc --noEmit   # TypeScript compilation check

# Component testing
npm test           # Run Jest tests for components

# Expected: Clean build, no TypeScript errors, tests pass
```

### Level 4: Integration Testing
```bash
# End-to-end pipeline test
cd scraper/
python focused_scraper.py   # Generate real deals.json

# Verify output
ls -la ../public/deals.json
cat ../public/deals.json | jq '.[] | select(.dataSource == "FAKE")' | wc -l
# Expected: 0 fake data entries

# Start React development server
cd ..
npm start

# Manual testing:
# 1. Verify deals load with real images from Amazon
# 2. Check that prices match current Amazon.ca prices
# 3. Test affiliate links redirect to Amazon.ca with correct tag
# 4. Verify responsive design on mobile and desktop
```

### Level 5: Production Deployment Test
```bash
# Build for production
npm run build

# Test static deployment
npx serve -s build -l 3000

# Verify in browser:
# - All deals display correctly
# - Images load from Amazon servers
# - Affiliate links work with savingsgurucc-20 tag
# - No fake data displayed anywhere
```

## Final Validation Checklist
- [ ] All tests pass: `pytest scraper/tests/ -v && npm test`
- [ ] No linting errors: `ruff check scraper/ && npx tsc --noEmit`
- [ ] PAAPI integration working: Real Canadian marketplace data retrieved
- [ ] Scraping fallback functional: Anti-bot measures effective
- [ ] Zero fake data: `jq '.[] | select(.dataSource | contains("fake"))' public/deals.json` returns empty
- [ ] Real Amazon images: All images load from amazon.ca or amazon.com domains
- [ ] Affiliate links working: All URLs contain savingsgurucc-20 tag
- [ ] Responsive design: Mobile and desktop layouts functional
- [ ] Performance: Lazy loading and smooth animations working
- [ ] Error handling: Graceful failures when API/scraping fails

---

## Anti-Patterns to Avoid
- ❌ Don't generate ANY fake data when APIs fail - skip the product instead
- ❌ Don't ignore PAAPI rate limits - implement proper throttling
- ❌ Don't use generic User-Agent for scraping - use full browser headers
- ❌ Don't hardcode API credentials - use environment variables with pydantic-settings
- ❌ Don't create fake product images - use Amazon images or skip product
- ❌ Don't ignore mobile responsiveness - design mobile-first
- ❌ Don't skip input validation - use Pydantic models for all data
- ❌ Don't commit environment variables or API keys to repository